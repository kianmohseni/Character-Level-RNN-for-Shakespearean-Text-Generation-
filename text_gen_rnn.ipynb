{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "484936aa-0ed4-4568-90df-f1cb675c8b79",
      "metadata": {
        "id": "484936aa-0ed4-4568-90df-f1cb675c8b79"
      },
      "source": [
        "## <font color='blue'>Text generation using a Char-RNN model</font>\n",
        "\n",
        "We're going to train a Recurrent Neural Network (RNN) to understand and generate text character by character. To do this, we'll provide the RNN with a large piece of text and ask it to learn the likelihood of the next character based on the sequence of previous characters.\n",
        "\n",
        "Let's break it down with a simple example: Imagine our vocabulary consists of just four letters, \"helo,\" and our training sequence is \"hello.\" In this case, we have four separate training examples:\n",
        "\n",
        "- The RNN should learn that when it sees \"h\", the next character \"e\" is likely.\n",
        "- When it encounters \"he\", it should expect \"l\" to come next.\n",
        "- Similarly, when it has \"hel\" as input, it should predict \"l\".\n",
        "- Finally, after \"hell\", it should anticipate \"o\".\n",
        "\n",
        "To make this happen, we'll represent each character as a vector using a technique called 1-of-k encoding, where each character is uniquely identified by a specific position in the vector. We'll then feed these character vectors into the RNN one at a time using a step function. The RNN will produce a sequence of output vectors, each with four dimensions, corresponding to the likelihood of the next character in the sequence.\n",
        "\n",
        "In essence, we're training the RNN to understand and generate text character by character, and it will predict the next character based on the context of the preceding characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2998083e-0a1d-46e9-b6d7-b8d695f34736",
      "metadata": {
        "id": "2998083e-0a1d-46e9-b6d7-b8d695f34736"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import string\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f9accd2",
      "metadata": {
        "id": "8f9accd2"
      },
      "source": [
        "### <font color='blue'>Some pre-processing</font>\n",
        "\n",
        "We will train our model using a text file of Shakespeare's plays.\n",
        "\n",
        "The first step is create a mapping from characters to integers, so as to represent each string as a list of integers. This is essential since we can only pass in numbers to our model, not strings or characters. Using this mapping, we now have our corpus of text mapped into a list of numbers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Jv-UjYmCON",
        "outputId": "651c2590-1a46-4551-9154-05c7c21749ee"
      },
      "id": "22Jv-UjYmCON",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3fce75",
      "metadata": {
        "id": "2f3fce75"
      },
      "outputs": [],
      "source": [
        "# Create a character-to-index and index-to-character mapping\n",
        "chars = np.load('/content/drive/My Drive/DSC 257R/rnn-gen/chars.npy')\n",
        "# np.save('chars.npy', chars)\n",
        "char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "index_to_char = {i: char for i, char in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouHlR-bammVM",
        "outputId": "7b255750-f25c-4e80-c079-020c825b4e76"
      },
      "id": "ouHlR-bammVM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '&': 3,\n",
              " \"'\": 4,\n",
              " ',': 5,\n",
              " '-': 6,\n",
              " '.': 7,\n",
              " '3': 8,\n",
              " ':': 9,\n",
              " ';': 10,\n",
              " '?': 11,\n",
              " 'A': 12,\n",
              " 'B': 13,\n",
              " 'C': 14,\n",
              " 'D': 15,\n",
              " 'E': 16,\n",
              " 'F': 17,\n",
              " 'G': 18,\n",
              " 'H': 19,\n",
              " 'I': 20,\n",
              " 'J': 21,\n",
              " 'K': 22,\n",
              " 'L': 23,\n",
              " 'M': 24,\n",
              " 'N': 25,\n",
              " 'O': 26,\n",
              " 'P': 27,\n",
              " 'Q': 28,\n",
              " 'R': 29,\n",
              " 'S': 30,\n",
              " 'T': 31,\n",
              " 'U': 32,\n",
              " 'V': 33,\n",
              " 'W': 34,\n",
              " 'X': 35,\n",
              " 'Y': 36,\n",
              " 'Z': 37,\n",
              " '[': 38,\n",
              " ']': 39,\n",
              " 'a': 40,\n",
              " 'b': 41,\n",
              " 'c': 42,\n",
              " 'd': 43,\n",
              " 'e': 44,\n",
              " 'f': 45,\n",
              " 'g': 46,\n",
              " 'h': 47,\n",
              " 'i': 48,\n",
              " 'j': 49,\n",
              " 'k': 50,\n",
              " 'l': 51,\n",
              " 'm': 52,\n",
              " 'n': 53,\n",
              " 'o': 54,\n",
              " 'p': 55,\n",
              " 'q': 56,\n",
              " 'r': 57,\n",
              " 's': 58,\n",
              " 't': 59,\n",
              " 'u': 60,\n",
              " 'v': 61,\n",
              " 'w': 62,\n",
              " 'x': 63,\n",
              " 'y': 64,\n",
              " 'z': 65}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0419ca4f",
      "metadata": {
        "id": "0419ca4f"
      },
      "source": [
        "Let's examine the mapping between integers and characters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***We are considering 66 different characters and the integer code for 'A' is 12.***"
      ],
      "metadata": {
        "id": "Amvk76srmzEV"
      },
      "id": "Amvk76srmzEV"
    },
    {
      "cell_type": "markdown",
      "id": "5afc40bf",
      "metadata": {
        "id": "5afc40bf"
      },
      "source": [
        "Now let's read in Shakespeare's plays and convert the text to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc182e56",
      "metadata": {
        "id": "bc182e56"
      },
      "outputs": [],
      "source": [
        "text = open('/content/drive/My Drive/DSC 257R/rnn-gen/shakespeare_plays.txt', 'r').read()\n",
        "\n",
        "# Convert the text to a numerical sequence\n",
        "# text_as_int = [char_to_index[char] for char in text]\n",
        "\n",
        "data = list(text)\n",
        "for i, ch in enumerate(data):\n",
        "    data[i] = char_to_index[ch]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# data tensor on device\n",
        "data = torch.tensor(data).to(device)\n",
        "data = torch.unsqueeze(data, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_length = len(text)\n",
        "print(corpus_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jblRBFY9o2vG",
        "outputId": "b6fcdf62-6965-4b0d-fb0a-31965c7c4818"
      },
      "id": "jblRBFY9o2vG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3801088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oOXy9MTDqWMI"
      },
      "id": "oOXy9MTDqWMI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***The length of the corpus in characters is 3,801,088.***"
      ],
      "metadata": {
        "id": "qlWhhjn0o4gG"
      },
      "id": "qlWhhjn0o4gG"
    },
    {
      "cell_type": "markdown",
      "id": "a6c2c7b8",
      "metadata": {
        "id": "a6c2c7b8"
      },
      "source": [
        "### <font color='blue'>Defining our model</font>\n",
        "\n",
        "##### Initialization:\n",
        "\n",
        "  The `__init__` method initializes the RNN model with the following parameters:\n",
        "  - input_size: The size of the character vocabulary. This indicates the number of unique characters that the model can work with.\n",
        "  - output_size: The size of the output vocabulary. It's typically set to the same value as input_size for character generation tasks.\n",
        "  - hidden_size: The number of hidden units in the LSTM (Long Short-Term Memory) layer.\n",
        "  - num_layers: The number of LSTM layers stacked on top of each other.\n",
        "\n",
        "##### Embedding Layer:\n",
        "\n",
        "  Inside the `__init__` method, an `nn.Embedding` layer is created. This layer is used to convert character indices (input) into dense vectors of fixed size.\n",
        "\n",
        "##### LSTM Layer:\n",
        "\n",
        "The `nn.LSTM layer` is defined with the specified `input_size`, `hidden_size`, and `num_layers`. This LSTM layer will process the embedded character sequence to capture dependencies and patterns within the sequence.\n",
        "\n",
        "##### Decoder Layer:\n",
        "\n",
        "After the LSTM layer, there is a linear (fully connected) layer defined as `nn.Linear`, which takes the output from the LSTM layer and maps it to the desired output size.\n",
        "\n",
        "##### Forward Pass:\n",
        "\n",
        "The forward method is where the actual computation occurs. It takes an input sequence (`input_seq`) and a hidden state (`hidden_state`) as input arguments.\n",
        "\n",
        "First, the input sequence is passed through the embedding layer to convert the character indices into dense embeddings.\n",
        "\n",
        "Then, these embeddings are fed into the LSTM layer, which processes the sequence. The LSTM layer produces an output sequence (output) and an updated hidden state.\n",
        "\n",
        "Finally, the output from the LSTM is passed through the linear decoder layer to generate the predictions for the next characters in the sequence.\n",
        "\n",
        "The forward method returns the output sequence and the updated hidden state.\n",
        "\n",
        "Note that the `self.rnn` is actually an LSTM. This is used since LSTM's are known to outperform RNNs in most language tasks. We can very well replace this with an RNN, but would expect the model not to perform that well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55ed58e",
      "metadata": {
        "id": "b55ed58e"
      },
      "outputs": [],
      "source": [
        "# Define the Char-RNN Model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, input_size)\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embedding = self.embedding(input_seq)\n",
        "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84afd463",
      "metadata": {
        "id": "84afd463"
      },
      "source": [
        "### <font color='blue'>Defining a dataset class</font>\n",
        "\n",
        "In this part of the tutorial, we'll create a custom PyTorch dataset called `TextDataset`. This dataset is designed for training character-level text generation models like CharRNN. The dataset allows you to prepare your text data for training by converting characters to integer indices and creating input-target pairs for the model.\n",
        "\n",
        "\n",
        "\n",
        "##### Initialization:\n",
        "\n",
        "Accepts three parameters: `text`, `seq_length`, and `char_to_index`.\n",
        "\n",
        "- `text`: The input text data you want to train the model on.\n",
        "- `seq_length`: The length of sequences to be used during training (e.g., 50 characters per sequence).\n",
        "- `char_to_index`: A dictionary mapping characters to integer indices.\n",
        "\n",
        "##### Conversion of Text to Integers:\n",
        "\n",
        "Inside the constructor, the input text is converted into an integer representation by mapping characters to their corresponding integer indices using the `char_to_index` dictionary.\n",
        "\n",
        "##### `__len__`:\n",
        "\n",
        "Defines the length of the dataset. You can specify a fixed length (e.g., 10,000) for your dataset, but this can be adjusted based on your dataset size. What you can also do is simply set length as `len(text) - self.seq_length`. This would result in a much larger set of samples and you wouldn't need to randomly sample an index (as described next).\n",
        "\n",
        "##### `__getitem__`:\n",
        "\n",
        "\n",
        "Retrieves individual training examples from the dataset.\n",
        "\n",
        "- Randomly selects a starting index within the range `[0, len(text) - seq_length)` for each training example.\n",
        "- Creates an input sequence (`input_seq`) containing characters from the selected `index` to `index + seq_length`.\n",
        "- Creates a target sequence (`target_seq`) containing characters from `index + 1` to `index + seq_length + 1`.\n",
        "- Returns a tuple with `input_seq` and `target_seq`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "258ddeef",
      "metadata": {
        "id": "258ddeef"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length, char_to_index):\n",
        "        self.seq_length = seq_length\n",
        "        self.char_to_index = char_to_index\n",
        "        self.text_as_int = [char_to_index[char] for char in text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = random.randint(0, len(self.text_as_int) - self.seq_length)\n",
        "        input_seq = torch.tensor(self.text_as_int[idx:idx + self.seq_length])\n",
        "        target_seq = torch.tensor(self.text_as_int[idx + 1:idx + self.seq_length + 1])\n",
        "        return input_seq, target_seq\n",
        "\n",
        "# Create the dataset\n",
        "seq_length = 100\n",
        "text_dataset = TextDataset(text, seq_length, char_to_index)\n",
        "\n",
        "# Create a data loader\n",
        "batch_size = 2048\n",
        "data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c83d91",
      "metadata": {
        "id": "32c83d91"
      },
      "outputs": [],
      "source": [
        "# Define the training loop\n",
        "\n",
        "input_size = len(chars)\n",
        "output_size = len(chars)\n",
        "hidden_size = 512\n",
        "num_layers = 3\n",
        "\n",
        "model = CharRNN(input_size, output_size, hidden_size, num_layers)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 15\n",
        "\n",
        "# Training device\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9458b684",
      "metadata": {
        "id": "9458b684"
      },
      "source": [
        "We can now train our model using the following code. For ease of use, a pre-trained model has been provided since training the model can be a long process especially if you don't have GPUs set up on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d9cb45",
      "metadata": {
        "id": "87d9cb45"
      },
      "outputs": [],
      "source": [
        "## NO NEED TO RUN THIS CELL\n",
        "\n",
        "# for i_epoch in range(1, num_epochs+1):\n",
        "\n",
        "#     n = 0\n",
        "#     running_loss = 0\n",
        "#     hidden_state = None\n",
        "\n",
        "#     for i_data,(input_seq, target_seq) in enumerate(data_loader):\n",
        "#         print(i_data)\n",
        "#         # forward pass\n",
        "#         input_seq = input_seq.to(device)\n",
        "#         target_seq = target_seq.to(device)\n",
        "#         output, hidden_state = model(input_seq, hidden_state)\n",
        "#         print(output.shape,target_seq.shape)\n",
        "#         # compute loss\n",
        "#         loss = criterion(output.view(-1,output_size), target_seq.view(-1))\n",
        "#         running_loss += loss.item()\n",
        "\n",
        "#         # compute gradients and take optimizer step\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         n +=1\n",
        "\n",
        "\n",
        "#     # print loss and save weights after every epoch\n",
        "#     print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss/n))\n",
        "#     torch.save(model.state_dict(), './model_{}.pth'.format(i_epoch))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac16d96",
      "metadata": {
        "id": "3ac16d96"
      },
      "source": [
        "Let's load the pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004ed09c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "004ed09c",
        "outputId": "62954437-fc1d-4706-c0b8-a94698e2e2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-654406fff035>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/My Drive/DSC 257R/rnn-gen/CharRNN_shakespeare.pth',map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharRNN(\n",
              "  (embedding): Embedding(66, 66)\n",
              "  (rnn): LSTM(66, 512, num_layers=3)\n",
              "  (decoder): Linear(in_features=512, out_features=66, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/My Drive/DSC 257R/rnn-gen/CharRNN_shakespeare.pth',map_location=torch.device('cpu')))\n",
        "model = model.cpu()\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The input and output sizes are both set to 66 because there are 66 unique characters in the dataset and the model uses one-hot encoding for each character when training and generating text"
      ],
      "metadata": {
        "id": "6D7Q_oB2qABZ"
      },
      "id": "6D7Q_oB2qABZ"
    },
    {
      "cell_type": "markdown",
      "id": "2a61cca4",
      "metadata": {
        "id": "2a61cca4"
      },
      "source": [
        "Time to generate some Shakespeare!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f1107b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7f1107b",
        "outputId": "8a075195-4c53-48de-cf4f-66ed712c4f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ly. Some man\n",
            "so far in England, comes she will speak with thee with most\n",
            "friends your countryman, and this hands, your\n",
            "great weaking have sometimes.\n",
            "Your husband, and sit my hand,\n",
            "Of England, alile, take nestrous us Glouceta, if ever cast\n",
            "and fly to be in two host and flow\n",
            "and valiantry, the king is claim: I have ago,\n",
            "When I unlucked on; and beseech his hand:\n",
            "Divoe, people, we shall so Charles saw their highness.\n",
            "\n",
            "KING HENRY V:\n",
            "Whas is thy broth and foods of gold, and leity,\n",
            "Offered in a just gracious lustness between\n",
            "his friends, his bowlards: and in this passageeest\n",
            "sickle like too true, some converse of ill-blown\n",
            "Follow Anne Sly gloven; Fridite, who can keep\n",
            "this rich chafe; leaving in beauty in a frame,\n",
            "In reproof all my dewn of terriem, box'd it hence;\n",
            "So covering out of them to be known up\n",
            "The glove of your worch, asberging of monmouth,\n",
            "Or their unnation'd retracted money false;\n",
            "For I have follow thy brothers in dukedom\n",
            "The swaggog?\n",
            "\n",
            "PISTOL:\n",
            "Captain, assure ye, do it alexant me.\n",
            "\n",
            "KING HENRY V:\n",
            "I had rather be,\n",
            "And those day gave them more, to say maw show into good night.\n",
            "\n",
            "ORLEANS:\n",
            "What, as came in unhapping coz!\n",
            "\n",
            "FLUELLEN:\n",
            "Contunp me each other fourteen France, ip on\n",
            "his fooly lack, that's bounty glove, flowers in\n",
            "yours.\n",
            "\n",
            "DAUPHIN:\n",
            "A net of answer is a show in sweet\n",
            "Stuff and vill improvisible.\n",
            "Captain?\n",
            "\n",
            "DONEN:\n",
            "No, morrow, and your safe sireth prass the issue;\n",
            "Bound thy play year to ravel fairy: thereeff, Clean is\n",
            "wipelly and her own flowerf of blood;\n",
            "Be gorged indied. Let me flesh him: it is\n",
            "fair, as I alone in quavailics.\n",
            "\n",
            "ALICE:\n",
            "As I am alilot.\n",
            "\n",
            "KING HENRY V:\n",
            "Quilotrezo!\n",
            "\n",
            "BIRY:\n",
            "\n",
            "KING HENRY V:\n",
            "Four king, look, how indirected you, madam?\n",
            "let us prison Thome! Adieu: I must poor hand\n",
            "in the ear, and sound, thou hast his curses from French\n",
            "And try to thy under: if we can die.\n",
            "\n",
            "KING HENRY V:\n",
            "Good brother, I know not fresh, and on a\n",
            "blood, I beseech your lord, nor in cloy'd master,\n",
            "and feast with prow in man.\n",
            "\n",
            "Boy:\n",
            "'Tis frail appulters, boys and me to the boy,\n",
            "Emb"
          ]
        }
      ],
      "source": [
        "input_seq = data[25:26].cpu()\n",
        "hidden_state = None\n",
        "o_len = 0\n",
        "output_len = 2000\n",
        "while o_len < output_len:\n",
        "    # forward pass\n",
        "    output, hidden_state = model(input_seq, hidden_state)\n",
        "    # construct categorical distribution and sample a character\n",
        "    output = torch.nn.functional.softmax(torch.squeeze(output), dim=0)\n",
        "    dist = torch.distributions.Categorical(output)\n",
        "    index = dist.sample()\n",
        "    # index = torch.argmax(output)\n",
        "    # print the sampled character\n",
        "    print(index_to_char[index.item()], end='')\n",
        "\n",
        "    # next input is current output\n",
        "    input_seq[0][0] = index.item()\n",
        "    o_len += 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "189px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}